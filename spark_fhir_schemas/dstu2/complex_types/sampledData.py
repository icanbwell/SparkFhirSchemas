from typing import Union, List, Optional

from pyspark.sql.types import StructType, StructField, StringType, DataType


# This file is auto-generated by generate_schema so do not edit manually
# noinspection PyPep8Naming
class SampledDataSchema:
    """
    A series of measurements taken by a device, with upper and lower limits. There
    may be more than one dimension in the data.
    If the element is present, it must have a value for at least one of the
    defined elements, an @id referenced from the Narrative, or extensions
    """

    # noinspection PyDefaultArgument
    @staticmethod
    def get_schema(
        max_nesting_depth: Optional[int] = 6,
        nesting_depth: int = 0,
        nesting_list: List[str] = [],
        max_recursion_limit: Optional[int] = 2,
        include_extension: Optional[bool] = False,
        extension_fields: Optional[List[str]] = [
            "valueBoolean",
            "valueCode",
            "valueDate",
            "valueDateTime",
            "valueDecimal",
            "valueId",
            "valueInteger",
            "valuePositiveInt",
            "valueString",
            "valueTime",
            "valueUnsignedInt",
            "valueUri",
            "valueQuantity",
        ],
        extension_depth: int = 0,
        max_extension_depth: Optional[int] = 2,
    ) -> Union[StructType, DataType]:
        """
        A series of measurements taken by a device, with upper and lower limits. There
        may be more than one dimension in the data.
        If the element is present, it must have a value for at least one of the
        defined elements, an @id referenced from the Narrative, or extensions


            id: None
            extension: May be used to represent additional information that is not part of the basic
        definition of the element. In order to make the use of extensions safe and
        manageable, there is a strict set of governance  applied to the definition and
        use of extensions. Though any implementer is allowed to define an extension,
        there is a set of requirements that SHALL be met as part of the definition of
        the extension.
            origin: The base quantity that a measured value of zero represents. In addition, this
        provides the units of the entire measurement series.
            period: The length of time between sampling times, measured in milliseconds.
            factor: A correction factor that is applied to the sampled data points before they are
        added to the origin.
            lowerLimit: The lower limit of detection of the measured points. This is needed if any of
        the data points have the value "L" (lower than detection limit).
            upperLimit: The upper limit of detection of the measured points. This is needed if any of
        the data points have the value "U" (higher than detection limit).
            dimensions: The number of sample points at each time point. If this value is greater than
        one, then the dimensions will be interlaced - all the sample points for a
        point in time will be recorded at once.
            data: A series of data points which are decimal values separated by a single space
        (character u20). The special values "E" (error), "L" (below detection limit)
        and "U" (above detection limit) can also be used in place of a decimal value.
        """
        if (
            max_recursion_limit
            and nesting_list.count("SampledData") >= max_recursion_limit
        ) or (max_nesting_depth and nesting_depth >= max_nesting_depth):
            return StructType([StructField("id", StringType(), True)])
        # Return at least one field in the struct or Spark throws an error
        # "Datasource does not support writing empty or nested empty schemas"
        return StructType([StructField("id", StringType(), True)])
