from typing import Union, List, Optional

from pyspark.sql.types import StructType, StructField, StringType, DataType


# This file is auto-generated by generate_schema so do not edit manually
# noinspection PyPep8Naming
class AttachmentSchema:
    """
    For referring to data content defined in other formats.
    If the element is present, it must have a value for at least one of the
    defined elements, an @id referenced from the Narrative, or extensions
    """

    # noinspection PyDefaultArgument
    @staticmethod
    def get_schema(
        max_nesting_depth: Optional[int] = 6,
        nesting_depth: int = 0,
        nesting_list: List[str] = [],
        max_recursion_limit: Optional[int] = 2,
        include_extension: Optional[bool] = False,
        extension_fields: Optional[List[str]] = [
            "valueBoolean",
            "valueCode",
            "valueDate",
            "valueDateTime",
            "valueDecimal",
            "valueId",
            "valueInteger",
            "valuePositiveInt",
            "valueString",
            "valueTime",
            "valueUnsignedInt",
            "valueUri",
            "valueQuantity",
        ],
        extension_depth: int = 0,
        max_extension_depth: Optional[int] = 2,
    ) -> Union[StructType, DataType]:
        """
        For referring to data content defined in other formats.
        If the element is present, it must have a value for at least one of the
        defined elements, an @id referenced from the Narrative, or extensions


            id: None
            extension: May be used to represent additional information that is not part of the basic
        definition of the element. In order to make the use of extensions safe and
        manageable, there is a strict set of governance  applied to the definition and
        use of extensions. Though any implementer is allowed to define an extension,
        there is a set of requirements that SHALL be met as part of the definition of
        the extension.
            contentType: Identifies the type of the data in the attachment and allows a method to be
        chosen to interpret or render the data. Includes mime type parameters such as
        charset where appropriate.
            language: The human language of the content. The value can be any valid value according
        to BCP 47.
            data: The actual data of the attachment - a sequence of bytes. In XML, represented
        using base64.
            url: An alternative location where the data can be accessed.
            size: The number of bytes of data that make up this attachment.
            hash: The calculated hash of the data using SHA-1. Represented using base64.
            title: A label or set of text to display in place of the data.
            creation: The date that the attachment was first created.
        """
        if (
            max_recursion_limit
            and nesting_list.count("Attachment") >= max_recursion_limit
        ) or (max_nesting_depth and nesting_depth >= max_nesting_depth):
            return StructType([StructField("id", StringType(), True)])
        # Return at least one field in the struct or Spark throws an error
        # "Datasource does not support writing empty or nested empty schemas"
        return StructType([StructField("id", StringType(), True)])
