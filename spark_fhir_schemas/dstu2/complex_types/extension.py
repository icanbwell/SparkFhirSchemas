from typing import Union, List, Optional

from pyspark.sql.types import StructType, StructField, StringType, DataType


# This file is auto-generated by generate_schema so do not edit manually
# noinspection PyPep8Naming
class ExtensionSchema:
    """
    Optional Extensions Element - found in all resources.
    If the element is present, it must have a value for at least one of the
    defined elements, an @id referenced from the Narrative, or extensions
    """

    # noinspection PyDefaultArgument
    @staticmethod
    def get_schema(
        max_nesting_depth: Optional[int] = 6,
        nesting_depth: int = 0,
        nesting_list: List[str] = [],
        max_recursion_limit: Optional[int] = 2,
        include_extension: Optional[bool] = False,
        extension_fields: Optional[List[str]] = [
            "valueBoolean",
            "valueCode",
            "valueDate",
            "valueDateTime",
            "valueDecimal",
            "valueId",
            "valueInteger",
            "valuePositiveInt",
            "valueString",
            "valueTime",
            "valueUnsignedInt",
            "valueUri",
            "valueQuantity",
        ],
        extension_depth: int = 0,
        max_extension_depth: Optional[int] = 2,
    ) -> Union[StructType, DataType]:
        """
        Optional Extensions Element - found in all resources.
        If the element is present, it must have a value for at least one of the
        defined elements, an @id referenced from the Narrative, or extensions


            id: None
            extension: May be used to represent additional information that is not part of the basic
        definition of the element. In order to make the use of extensions safe and
        manageable, there is a strict set of governance  applied to the definition and
        use of extensions. Though any implementer is allowed to define an extension,
        there is a set of requirements that SHALL be met as part of the definition of
        the extension.
            url: None
            valueBoolean: None
            valueInteger: None
            valueDecimal: None
            valueBase64Binary: None
            valueInstant: None
            valueString: None
            valueUri: None
            valueDate: None
            valueDateTime: None
            valueTime: None
            valueCode: None
            valueOid: None
            valueUuid: None
            valueId: None
            valueUnsignedInt: None
            valuePositiveInt: None
            valueMarkdown: None
            valueAnnotation: None
            valueAttachment: None
            valueIdentifier: None
            valueCodeableConcept: None
            valueCoding: None
            valueQuantity: None
            valueRange: None
            valuePeriod: None
            valueRatio: None
            valueReference: None
            valueSampledData: None
            valueSignature: None
            valueHumanName: None
            valueAddress: None
            valueContactPoint: None
            valueTiming: None
            valueMeta: None
        """
        if (
            max_recursion_limit
            and nesting_list.count("Extension") >= max_recursion_limit
        ) or (max_nesting_depth and nesting_depth >= max_nesting_depth):
            return StructType([StructField("id", StringType(), True)])
        if max_extension_depth and extension_depth >= max_extension_depth:
            return StructType([StructField("id", StringType(), True)])
        # Return at least one field in the struct or Spark throws an error
        # "Datasource does not support writing empty or nested empty schemas"
        return StructType([StructField("id", StringType(), True)])
