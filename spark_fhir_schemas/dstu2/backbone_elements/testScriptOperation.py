from typing import Union, List, Optional

from pyspark.sql.types import StructType, StructField, StringType, DataType


# This file is auto-generated by generate_schema so do not edit manually
# noinspection PyPep8Naming
class TestScriptOperationSchema:
    """
    TestScript is a resource that specifies a suite of tests against a FHIR server
    implementation to determine compliance against the FHIR specification.
    """

    # noinspection PyDefaultArgument
    @staticmethod
    def get_schema(
        max_nesting_depth: Optional[int] = 6,
        nesting_depth: int = 0,
        nesting_list: List[str] = [],
        max_recursion_limit: Optional[int] = 2,
        include_extension: Optional[bool] = False,
        extension_fields: Optional[List[str]] = [
            "valueBoolean",
            "valueCode",
            "valueDate",
            "valueDateTime",
            "valueDecimal",
            "valueId",
            "valueInteger",
            "valuePositiveInt",
            "valueString",
            "valueTime",
            "valueUnsignedInt",
            "valueUri",
            "valueQuantity",
        ],
        extension_depth: int = 0,
        max_extension_depth: Optional[int] = 2,
    ) -> Union[StructType, DataType]:
        """
        TestScript is a resource that specifies a suite of tests against a FHIR server
        implementation to determine compliance against the FHIR specification.


            id: None
            extension: May be used to represent additional information that is not part of the basic
        definition of the element. In order to make the use of extensions safe and
        manageable, there is a strict set of governance  applied to the definition and
        use of extensions. Though any implementer is allowed to define an extension,
        there is a set of requirements that SHALL be met as part of the definition of
        the extension.
            modifierExtension: May be used to represent additional information that is not part of the basic
        definition of the element, and that modifies the understanding of the element
        that contains it. Usually modifier elements provide negation or qualification.
        In order to make the use of extensions safe and manageable, there is a strict
        set of governance applied to the definition and use of extensions. Though any
        implementer is allowed to define an extension, there is a set of requirements
        that SHALL be met as part of the definition of the extension. Applications
        processing a resource are required to check for modifier extensions.
            type: Server interaction or operation type.
            resource: The type of the resource.  See http://hl7-fhir.github.io/resourcelist.html.
            label: The label would be used for tracking/logging purposes by test engines.
            description: The description would be used by test engines for tracking and reporting
        purposes.
            accept: The content-type or mime-type to use for RESTful operation in the 'Accept'
        header.
            contentType: The content-type or mime-type to use for RESTful operation in the 'Content-
        Type' header.
            destination: Which server to perform the operation on.
            encodeRequestUrl: Whether or not to implicitly send the request url in encoded format. The
        default is true to match the standard RESTful client behavior. Set to false
        when communicating with a server that does not support encoded url paths.
            params: Path plus parameters after [type].  Used to set parts of the request URL
        explicitly.
            requestHeader: Header elements would be used to set HTTP headers.
            responseId: The fixture id (maybe new) to map to the response.
            sourceId: The id of the fixture used as the body of a PUT or POST request.
            targetId: Id of fixture used for extracting the [id],  [type], and [vid] for GET
        requests.
            url: Complete request URL.
        """
        if (
            max_recursion_limit
            and nesting_list.count("TestScriptOperation") >= max_recursion_limit
        ) or (max_nesting_depth and nesting_depth >= max_nesting_depth):
            return StructType([StructField("id", StringType(), True)])
        # Return at least one field in the struct or Spark throws an error
        # "Datasource does not support writing empty or nested empty schemas"
        return StructType([StructField("id", StringType(), True)])
