from typing import Union, List, Optional

from pyspark.sql.types import StructType, StructField, StringType, DataType


# This file is auto-generated by generate_schema so do not edit manually
# noinspection PyPep8Naming
class ContractValuedItemSchema:
    """
    A formal agreement between parties regarding the conduct of business, exchange
    of information or other matters.
    """

    # noinspection PyDefaultArgument
    @staticmethod
    def get_schema(
        max_nesting_depth: Optional[int] = 6,
        nesting_depth: int = 0,
        nesting_list: List[str] = [],
        max_recursion_limit: Optional[int] = 2,
        include_extension: Optional[bool] = False,
        extension_fields: Optional[List[str]] = [
            "valueBoolean",
            "valueCode",
            "valueDate",
            "valueDateTime",
            "valueDecimal",
            "valueId",
            "valueInteger",
            "valuePositiveInt",
            "valueString",
            "valueTime",
            "valueUnsignedInt",
            "valueUri",
            "valueQuantity",
        ],
        extension_depth: int = 0,
        max_extension_depth: Optional[int] = 2,
    ) -> Union[StructType, DataType]:
        """
        A formal agreement between parties regarding the conduct of business, exchange
        of information or other matters.


            id: None
            extension: May be used to represent additional information that is not part of the basic
        definition of the element. In order to make the use of extensions safe and
        manageable, there is a strict set of governance  applied to the definition and
        use of extensions. Though any implementer is allowed to define an extension,
        there is a set of requirements that SHALL be met as part of the definition of
        the extension.
            modifierExtension: May be used to represent additional information that is not part of the basic
        definition of the element, and that modifies the understanding of the element
        that contains it. Usually modifier elements provide negation or qualification.
        In order to make the use of extensions safe and manageable, there is a strict
        set of governance applied to the definition and use of extensions. Though any
        implementer is allowed to define an extension, there is a set of requirements
        that SHALL be met as part of the definition of the extension. Applications
        processing a resource are required to check for modifier extensions.
            entityCodeableConcept: None
            entityReference: None
            identifier: Identifies a Contract Valued Item instance.
            effectiveTime: Indicates the time during which this Contract ValuedItem information is
        effective.
            quantity: Specifies the units by which the Contract Valued Item is measured or counted,
        and quantifies the countable or measurable Contract Valued Item instances.
            unitPrice: A Contract Valued Item unit valuation measure.
            factor: A real number that represents a multiplier used in determining the overall
        value of the Contract Valued Item delivered. The concept of a Factor allows
        for a discount or surcharge multiplier to be applied to a monetary amount.
            points: An amount that expresses the weighting (based on difficulty, cost and/or
        resource intensiveness) associated with the Contract Valued Item delivered.
        The concept of Points allows for assignment of point values for a Contract
        Valued Item, such that a monetary amount can be assigned to each point.
            net: Expresses the product of the Contract Valued Item unitQuantity and the
        unitPriceAmt. For example, the formula: unit Quantity * unit Price (Cost per
        Point) * factor Number  * points = net Amount. Quantity, factor and points are
        assumed to be 1 if not supplied.
        """
        if (
            max_recursion_limit
            and nesting_list.count("ContractValuedItem") >= max_recursion_limit
        ) or (max_nesting_depth and nesting_depth >= max_nesting_depth):
            return StructType([StructField("id", StringType(), True)])
        # Return at least one field in the struct or Spark throws an error
        # "Datasource does not support writing empty or nested empty schemas"
        return StructType([StructField("id", StringType(), True)])
